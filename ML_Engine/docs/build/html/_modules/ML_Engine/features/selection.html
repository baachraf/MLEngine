<!DOCTYPE html>

<html lang="en" data-content_root="../../../">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>ML_Engine.features.selection &#8212; ML Engine 0.0.1 documentation</title>
    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=5ecbeea2" />
    <link rel="stylesheet" type="text/css" href="../../../_static/basic.css?v=686e5160" />
    <link rel="stylesheet" type="text/css" href="../../../_static/alabaster.css?v=27fed22d" />
    <script src="../../../_static/documentation_options.js?v=d45e8c67"></script>
    <script src="../../../_static/doctools.js?v=9bcbadda"></script>
    <script src="../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
   
  <link rel="stylesheet" href="../../../_static/custom.css" type="text/css" />
  

  
  

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <h1>Source code for ML_Engine.features.selection</h1><div class="highlight"><pre>
<span></span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Feature Selection and Engineering</span>
<span class="sd">==================================</span>

<span class="sd">Functions for feature selection, engineering, and dimensionality reduction.</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">warnings</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.feature_selection</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">SelectKBest</span><span class="p">,</span> <span class="n">mutual_info_classif</span><span class="p">,</span> <span class="n">mutual_info_regression</span><span class="p">,</span>
    <span class="n">f_classif</span><span class="p">,</span> <span class="n">f_regression</span><span class="p">,</span> <span class="n">chi2</span><span class="p">,</span> <span class="n">RFE</span><span class="p">,</span> <span class="n">VarianceThreshold</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">PolynomialFeatures</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.decomposition</span><span class="w"> </span><span class="kn">import</span> <span class="n">PCA</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.ensemble</span><span class="w"> </span><span class="kn">import</span> <span class="n">RandomForestClassifier</span><span class="p">,</span> <span class="n">RandomForestRegressor</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.linear_model</span><span class="w"> </span><span class="kn">import</span> <span class="n">LinearRegression</span><span class="p">,</span> <span class="n">LogisticRegression</span>
<span class="c1"># Optional imports for advanced feature selection methods</span>
<span class="k">try</span><span class="p">:</span>
    <span class="kn">import</span><span class="w"> </span><span class="nn">xgboost</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">xgb</span>
    <span class="n">XGBOOST_AVAILABLE</span> <span class="o">=</span> <span class="kc">True</span>
<span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
    <span class="n">xgb</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">XGBOOST_AVAILABLE</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s1">&#39;xgboost not installed. Some feature selection methods may not be available.&#39;</span><span class="p">)</span>

<span class="k">try</span><span class="p">:</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">boruta</span><span class="w"> </span><span class="kn">import</span> <span class="n">BorutaPy</span>
    <span class="n">BORUTA_AVAILABLE</span> <span class="o">=</span> <span class="kc">True</span>
<span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
    <span class="n">BorutaPy</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">BORUTA_AVAILABLE</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s1">&#39;boruta not installed. Boruta feature selection will not be available.&#39;</span><span class="p">)</span>


<div class="viewcode-block" id="variance_feature_selection">
<a class="viewcode-back" href="../../../ML_Engine.features.html#ML_Engine.features.selection.variance_feature_selection">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">variance_feature_selection</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="mf">0.0</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Select features based on variance threshold.</span>
<span class="sd">    </span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    X : array-like or pandas.DataFrame</span>
<span class="sd">        Feature data</span>
<span class="sd">    threshold : float, default=0.0</span>
<span class="sd">        Variance threshold</span>
<span class="sd">    </span>
<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    selected_features : array-like</span>
<span class="sd">        Indices of selected features</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">selector</span> <span class="o">=</span> <span class="n">VarianceThreshold</span><span class="p">(</span><span class="n">threshold</span><span class="o">=</span><span class="n">threshold</span><span class="p">)</span>
    <span class="n">selector</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">selector</span><span class="o">.</span><span class="n">get_support</span><span class="p">(</span><span class="n">indices</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></div>


<div class="viewcode-block" id="select_k_best_features">
<a class="viewcode-back" href="../../../ML_Engine.features.html#ML_Engine.features.selection.select_k_best_features">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">select_k_best_features</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">score_func</span><span class="o">=</span><span class="s1">&#39;mutual_info&#39;</span><span class="p">,</span> <span class="n">problem_type</span><span class="o">=</span><span class="s1">&#39;classification&#39;</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Select k best features using scoring function.</span>
<span class="sd">    </span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    X : array-like or pandas.DataFrame</span>
<span class="sd">        Feature data</span>
<span class="sd">    y : array-like</span>
<span class="sd">        Target data</span>
<span class="sd">    k : int, default=10</span>
<span class="sd">        Number of features to select</span>
<span class="sd">    score_func : {&#39;mutual_info&#39;, &#39;f_classif&#39;, &#39;f_regression&#39;, &#39;chi2&#39;}</span>
<span class="sd">        Scoring function</span>
<span class="sd">    problem_type : {&#39;classification&#39;, &#39;regression&#39;}</span>
<span class="sd">        Problem type</span>
<span class="sd">    </span>
<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    selected_features : array-like</span>
<span class="sd">        Indices of selected features</span>
<span class="sd">    scores : array-like</span>
<span class="sd">        Feature scores</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Map score function names to actual functions</span>
    <span class="n">score_funcs</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s1">&#39;mutual_info&#39;</span><span class="p">:</span> <span class="n">mutual_info_classif</span> <span class="k">if</span> <span class="n">problem_type</span> <span class="o">==</span> <span class="s1">&#39;classification&#39;</span> <span class="k">else</span> <span class="n">mutual_info_regression</span><span class="p">,</span>
        <span class="s1">&#39;f_classif&#39;</span><span class="p">:</span> <span class="n">f_classif</span><span class="p">,</span>
        <span class="s1">&#39;f_regression&#39;</span><span class="p">:</span> <span class="n">f_regression</span><span class="p">,</span>
        <span class="s1">&#39;chi2&#39;</span><span class="p">:</span> <span class="n">chi2</span>
    <span class="p">}</span>
    
    <span class="k">if</span> <span class="n">score_func</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">score_funcs</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unknown score function: </span><span class="si">{</span><span class="n">score_func</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    
    <span class="n">score_function</span> <span class="o">=</span> <span class="n">score_funcs</span><span class="p">[</span><span class="n">score_func</span><span class="p">]</span>
    
    <span class="c1"># Chi2 requires non-negative values</span>
    <span class="k">if</span> <span class="n">score_func</span> <span class="o">==</span> <span class="s1">&#39;chi2&#39;</span><span class="p">:</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">X</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">()</span><span class="o">.</span><span class="n">any</span><span class="p">():</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Chi-squared test requires non-negative values&quot;</span><span class="p">)</span>
    
    <span class="n">selector</span> <span class="o">=</span> <span class="n">SelectKBest</span><span class="p">(</span><span class="n">score_function</span><span class="o">=</span><span class="n">score_function</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="n">k</span><span class="p">)</span>
    <span class="n">selector</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">selector</span><span class="o">.</span><span class="n">get_support</span><span class="p">(</span><span class="n">indices</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span> <span class="n">selector</span><span class="o">.</span><span class="n">scores_</span></div>



<div class="viewcode-block" id="recursive_feature_elimination">
<a class="viewcode-back" href="../../../ML_Engine.features.html#ML_Engine.features.selection.recursive_feature_elimination">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">recursive_feature_elimination</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">estimator</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">n_features_to_select</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> 
                                  <span class="n">step</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">problem_type</span><span class="o">=</span><span class="s1">&#39;classification&#39;</span><span class="p">,</span> <span class="o">**</span><span class="n">estimator_params</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Recursive Feature Elimination.</span>
<span class="sd">    </span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    X : array-like or pandas.DataFrame</span>
<span class="sd">        Feature data</span>
<span class="sd">    y : array-like</span>
<span class="sd">        Target data</span>
<span class="sd">    estimator : object, optional</span>
<span class="sd">        Estimator to use. If None, defaults based on problem_type.</span>
<span class="sd">    n_features_to_select : int, optional</span>
<span class="sd">        Number of features to select. If None, half of features.</span>
<span class="sd">    step : int or float, default=1</span>
<span class="sd">        Number of features to remove at each iteration</span>
<span class="sd">    problem_type : {&#39;classification&#39;, &#39;regression&#39;}</span>
<span class="sd">        Problem type</span>
<span class="sd">    **estimator_params : dict</span>
<span class="sd">        Parameters for estimator</span>
<span class="sd">    </span>
<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    selected_features : array-like</span>
<span class="sd">        Indices of selected features</span>
<span class="sd">    rankings : array-like</span>
<span class="sd">        Feature rankings (1 = best)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">estimator</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">problem_type</span> <span class="o">==</span> <span class="s1">&#39;classification&#39;</span><span class="p">:</span>
            <span class="n">estimator</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span> <span class="o">**</span><span class="n">estimator_params</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">estimator</span> <span class="o">=</span> <span class="n">RandomForestRegressor</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span> <span class="o">**</span><span class="n">estimator_params</span><span class="p">)</span>
    
    <span class="k">if</span> <span class="n">n_features_to_select</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">n_features_to_select</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">//</span> <span class="mi">2</span>
    
    <span class="n">selector</span> <span class="o">=</span> <span class="n">RFE</span><span class="p">(</span><span class="n">estimator</span><span class="o">=</span><span class="n">estimator</span><span class="p">,</span> <span class="n">n_features_to_select</span><span class="o">=</span><span class="n">n_features_to_select</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="n">step</span><span class="p">)</span>
    <span class="n">selector</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">selector</span><span class="o">.</span><span class="n">get_support</span><span class="p">(</span><span class="n">indices</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span> <span class="n">selector</span><span class="o">.</span><span class="n">ranking_</span></div>



<div class="viewcode-block" id="get_rfe_result">
<a class="viewcode-back" href="../../../ML_Engine.features.html#ML_Engine.features.selection.get_rfe_result">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">get_rfe_result</span><span class="p">(</span><span class="n">estimator</span><span class="p">,</span> <span class="n">estimator_params</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">step</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">feature_names</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Perform Recursive Feature Elimination (RFE) and return selected features.</span>
<span class="sd">    </span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    estimator : sklearn estimator class</span>
<span class="sd">        Estimator to use (e.g., RandomForestClassifier)</span>
<span class="sd">    estimator_params : dict</span>
<span class="sd">        Parameters for the estimator</span>
<span class="sd">    k : int</span>
<span class="sd">        Number of features to select</span>
<span class="sd">    step : int or float</span>
<span class="sd">        Number/percentage of features to remove at each iteration</span>
<span class="sd">    X_train : array-like</span>
<span class="sd">        Training features</span>
<span class="sd">    y_train : array-like</span>
<span class="sd">        Training target</span>
<span class="sd">    feature_names : list</span>
<span class="sd">        Names of features</span>
<span class="sd">    </span>
<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    selected_features : list</span>
<span class="sd">        Names of selected features</span>
<span class="sd">    selector : RFE</span>
<span class="sd">        Fitted RFE selector</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.feature_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">RFE</span>
    
    <span class="n">estimator_model</span> <span class="o">=</span> <span class="n">estimator</span><span class="p">(</span><span class="o">**</span><span class="n">estimator_params</span><span class="p">)</span>
    <span class="n">selector</span> <span class="o">=</span> <span class="n">RFE</span><span class="p">(</span><span class="n">estimator</span><span class="o">=</span><span class="n">estimator_model</span><span class="p">,</span> <span class="n">n_features_to_select</span><span class="o">=</span><span class="n">k</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="n">step</span><span class="p">)</span>
    <span class="n">selector</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    
    <span class="n">selected_feature_mask</span> <span class="o">=</span> <span class="n">selector</span><span class="o">.</span><span class="n">get_support</span><span class="p">()</span>
    <span class="n">selected_features</span> <span class="o">=</span> <span class="p">[</span><span class="n">feature_names</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">feature_names</span><span class="p">))</span> 
                         <span class="k">if</span> <span class="n">selected_feature_mask</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span>
    
    <span class="k">return</span> <span class="n">selected_features</span><span class="p">,</span> <span class="n">selector</span></div>



<div class="viewcode-block" id="boruta_feature_selection">
<a class="viewcode-back" href="../../../ML_Engine.features.html#ML_Engine.features.selection.boruta_feature_selection">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">boruta_feature_selection</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">problem_type</span><span class="o">=</span><span class="s1">&#39;classification&#39;</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Boruta all-relevant feature selection.</span>
<span class="sd">    </span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    X : array-like or pandas.DataFrame</span>
<span class="sd">        Feature data</span>
<span class="sd">    y : array-like</span>
<span class="sd">        Target data</span>
<span class="sd">    problem_type : {&#39;classification&#39;, &#39;regression&#39;}</span>
<span class="sd">        Problem type</span>
<span class="sd">    max_iter : int, default=100</span>
<span class="sd">        Maximum number of iterations</span>
<span class="sd">    random_state : int, default=42</span>
<span class="sd">        Random seed</span>
<span class="sd">    </span>
<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    selected_features : array-like</span>
<span class="sd">        Indices of selected features</span>
<span class="sd">    feature_importances : array-like</span>
<span class="sd">        Feature importances</span>
<span class="sd">    </span>
<span class="sd">    Raises</span>
<span class="sd">    ------</span>
<span class="sd">    ImportError</span>
<span class="sd">        If boruta package is not installed</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">BORUTA_AVAILABLE</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ImportError</span><span class="p">(</span><span class="s1">&#39;boruta package is required for boruta_feature_selection&#39;</span><span class="p">)</span>
    
    <span class="c1"># Convert to numpy arrays if needed</span>
    <span class="n">X_arr</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">values</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">)</span> <span class="k">else</span> <span class="n">X</span>
    <span class="n">y_arr</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">values</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">)</span> <span class="k">else</span> <span class="n">y</span>
    
    <span class="c1"># Choose estimator based on problem type</span>
    <span class="k">if</span> <span class="n">problem_type</span> <span class="o">==</span> <span class="s1">&#39;classification&#39;</span><span class="p">:</span>
        <span class="n">estimator</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">estimator</span> <span class="o">=</span> <span class="n">RandomForestRegressor</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    
    <span class="c1"># Initialize Boruta</span>
    <span class="n">boruta</span> <span class="o">=</span> <span class="n">BorutaPy</span><span class="p">(</span>
        <span class="n">estimator</span><span class="o">=</span><span class="n">estimator</span><span class="p">,</span>
        <span class="n">n_estimators</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">,</span>
        <span class="n">max_iter</span><span class="o">=</span><span class="n">max_iter</span><span class="p">,</span>
        <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span>
        <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span>
    <span class="p">)</span>
    
    <span class="c1"># Fit Boruta</span>
    <span class="n">boruta</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_arr</span><span class="p">,</span> <span class="n">y_arr</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">boruta</span><span class="o">.</span><span class="n">support_</span><span class="p">,</span> <span class="n">boruta</span><span class="o">.</span><span class="n">importances_</span></div>



<div class="viewcode-block" id="calculate_polynomial_features_count">
<a class="viewcode-back" href="../../../ML_Engine.features.html#ML_Engine.features.selection.calculate_polynomial_features_count">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">calculate_polynomial_features_count</span><span class="p">(</span><span class="n">n_features</span><span class="p">,</span> <span class="n">degree</span><span class="p">,</span> <span class="n">interaction_only</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">include_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculate number of polynomial features.</span>
<span class="sd">    </span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    n_features : int</span>
<span class="sd">        Number of input features</span>
<span class="sd">    degree : int</span>
<span class="sd">        Polynomial degree</span>
<span class="sd">    interaction_only : bool, default=False</span>
<span class="sd">        Whether to include only interaction terms</span>
<span class="sd">    include_bias : bool, default=False</span>
<span class="sd">        Whether to include bias term</span>
<span class="sd">    </span>
<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    int</span>
<span class="sd">        Number of polynomial features</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Simple calculation - exact formula depends on implementation</span>
    <span class="c1"># This is approximate</span>
    <span class="k">if</span> <span class="n">interaction_only</span><span class="p">:</span>
        <span class="c1"># Only interaction terms</span>
        <span class="n">count</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">degree</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">d</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">count</span> <span class="o">+=</span> <span class="n">n_features</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># Combinations of features for interaction</span>
                <span class="c1"># This is simplified</span>
                <span class="n">count</span> <span class="o">+=</span> <span class="n">np</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">comb</span><span class="p">(</span><span class="n">n_features</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># All polynomial terms</span>
        <span class="n">count</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">comb</span><span class="p">(</span><span class="n">n_features</span> <span class="o">+</span> <span class="n">degree</span><span class="p">,</span> <span class="n">degree</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="mi">0</span> <span class="k">if</span> <span class="n">include_bias</span> <span class="k">else</span> <span class="mi">1</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">count</span></div>



<div class="viewcode-block" id="create_polynomial_features">
<a class="viewcode-back" href="../../../ML_Engine.features.html#ML_Engine.features.selection.create_polynomial_features">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">create_polynomial_features</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">degree</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">interaction_only</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">include_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
                               <span class="n">return_poly</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Create polynomial and interaction features.</span>
<span class="sd">    </span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    X : array-like or pandas.DataFrame</span>
<span class="sd">        Feature data</span>
<span class="sd">    degree : int, default=2</span>
<span class="sd">        Polynomial degree</span>
<span class="sd">    interaction_only : bool, default=False</span>
<span class="sd">        Whether to include only interaction terms</span>
<span class="sd">    include_bias : bool, default=False</span>
<span class="sd">        Whether to include bias term</span>
<span class="sd">    return_poly : bool, default=False</span>
<span class="sd">        Whether to return the PolynomialFeatures object</span>
<span class="sd">    </span>
<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    X_poly : array-like or pandas.DataFrame</span>
<span class="sd">        Polynomial features</span>
<span class="sd">    poly : PolynomialFeatures, optional</span>
<span class="sd">        Fitted PolynomialFeatures object (only if return_poly=True)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">poly</span> <span class="o">=</span> <span class="n">PolynomialFeatures</span><span class="p">(</span>
        <span class="n">degree</span><span class="o">=</span><span class="n">degree</span><span class="p">,</span>
        <span class="n">interaction_only</span><span class="o">=</span><span class="n">interaction_only</span><span class="p">,</span>
        <span class="n">include_bias</span><span class="o">=</span><span class="n">include_bias</span>
    <span class="p">)</span>
    
    <span class="n">X_poly</span> <span class="o">=</span> <span class="n">poly</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    
    <span class="c1"># Create column names if X is a DataFrame</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">):</span>
        <span class="n">feature_names</span> <span class="o">=</span> <span class="n">poly</span><span class="o">.</span><span class="n">get_feature_names_out</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
        <span class="n">X_poly</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">X_poly</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">feature_names</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">index</span><span class="p">)</span>
    
    <span class="k">if</span> <span class="n">return_poly</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">X_poly</span><span class="p">,</span> <span class="n">poly</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">X_poly</span></div>



<div class="viewcode-block" id="apply_polynomial_features">
<a class="viewcode-back" href="../../../ML_Engine.features.html#ML_Engine.features.selection.apply_polynomial_features">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">apply_polynomial_features</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">degree</span><span class="p">,</span> <span class="n">include_bias</span><span class="p">,</span> <span class="n">interaction_only</span><span class="p">,</span> 
                              <span class="n">selected_cols</span><span class="p">,</span> <span class="n">poly</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Apply polynomial feature transformation to selected columns.</span>
<span class="sd">    </span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    df : pandas.DataFrame</span>
<span class="sd">        Input DataFrame</span>
<span class="sd">    degree : int</span>
<span class="sd">        Polynomial degree</span>
<span class="sd">    include_bias : bool</span>
<span class="sd">        Whether to include bias term</span>
<span class="sd">    interaction_only : bool</span>
<span class="sd">        Whether to include only interaction terms</span>
<span class="sd">    selected_cols : list</span>
<span class="sd">        Columns to transform</span>
<span class="sd">    poly : PolynomialFeatures, optional</span>
<span class="sd">        Pre-fitted PolynomialFeatures object</span>
<span class="sd">    </span>
<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    transformed_df : pandas.DataFrame</span>
<span class="sd">        DataFrame with polynomial features</span>
<span class="sd">    poly : PolynomialFeatures</span>
<span class="sd">        Fitted PolynomialFeatures object</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">selected_cols</span><span class="p">]</span>
    
    <span class="k">if</span> <span class="n">poly</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">poly</span> <span class="o">=</span> <span class="n">PolynomialFeatures</span><span class="p">(</span>
            <span class="n">degree</span><span class="o">=</span><span class="n">degree</span><span class="p">,</span>
            <span class="n">include_bias</span><span class="o">=</span><span class="n">include_bias</span><span class="p">,</span>
            <span class="n">interaction_only</span><span class="o">=</span><span class="n">interaction_only</span>
        <span class="p">)</span>
        <span class="n">X_poly</span> <span class="o">=</span> <span class="n">poly</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">X_poly</span> <span class="o">=</span> <span class="n">poly</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    
    <span class="c1"># Create column names</span>
    <span class="n">feature_names</span> <span class="o">=</span> <span class="n">poly</span><span class="o">.</span><span class="n">get_feature_names_out</span><span class="p">(</span><span class="n">selected_cols</span><span class="p">)</span>
    
    <span class="c1"># Create new DataFrame with polynomial features</span>
    <span class="n">poly_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">X_poly</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">feature_names</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">df</span><span class="o">.</span><span class="n">index</span><span class="p">)</span>
    
    <span class="c1"># Drop original columns and concatenate polynomial features</span>
    <span class="n">transformed_df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="n">selected_cols</span><span class="p">)</span>
    <span class="n">transformed_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">transformed_df</span><span class="p">,</span> <span class="n">poly_df</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">transformed_df</span><span class="p">,</span> <span class="n">poly</span></div>



<div class="viewcode-block" id="select_features_by_method">
<a class="viewcode-back" href="../../../ML_Engine.features.html#ML_Engine.features.selection.select_features_by_method">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">select_features_by_method</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s1">&#39;variance&#39;</span><span class="p">,</span> <span class="n">problem_type</span><span class="o">=</span><span class="s1">&#39;classification&#39;</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Select features using specified method.</span>
<span class="sd">    </span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    X : array-like or pandas.DataFrame</span>
<span class="sd">        Feature data</span>
<span class="sd">    y : array-like</span>
<span class="sd">        Target data</span>
<span class="sd">    method : {&#39;variance&#39;, &#39;kbest&#39;, &#39;rfe&#39;, &#39;boruta&#39;, &#39;pca&#39;}</span>
<span class="sd">        Feature selection method</span>
<span class="sd">    problem_type : {&#39;classification&#39;, &#39;regression&#39;}</span>
<span class="sd">        Problem type</span>
<span class="sd">    **kwargs : dict</span>
<span class="sd">        Method-specific parameters</span>
<span class="sd">    </span>
<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    selected_indices : array-like</span>
<span class="sd">        Indices of selected features</span>
<span class="sd">    additional_info : dict</span>
<span class="sd">        Method-specific additional information</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">method</span> <span class="o">==</span> <span class="s1">&#39;variance&#39;</span><span class="p">:</span>
        <span class="n">threshold</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;threshold&#39;</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span>
        <span class="n">selected_indices</span> <span class="o">=</span> <span class="n">variance_feature_selection</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">threshold</span><span class="p">)</span>
        <span class="n">additional_info</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;method&#39;</span><span class="p">:</span> <span class="s1">&#39;variance&#39;</span><span class="p">,</span> <span class="s1">&#39;threshold&#39;</span><span class="p">:</span> <span class="n">threshold</span><span class="p">}</span>
    
    <span class="k">elif</span> <span class="n">method</span> <span class="o">==</span> <span class="s1">&#39;kbest&#39;</span><span class="p">:</span>
        <span class="n">k</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
        <span class="n">score_func</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;score_func&#39;</span><span class="p">,</span> <span class="s1">&#39;mutual_info&#39;</span><span class="p">)</span>
        <span class="n">selected_indices</span><span class="p">,</span> <span class="n">scores</span> <span class="o">=</span> <span class="n">select_k_best_features</span><span class="p">(</span>
            <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="n">k</span><span class="p">,</span> <span class="n">score_func</span><span class="o">=</span><span class="n">score_func</span><span class="p">,</span> <span class="n">problem_type</span><span class="o">=</span><span class="n">problem_type</span>
        <span class="p">)</span>
        <span class="n">additional_info</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s1">&#39;method&#39;</span><span class="p">:</span> <span class="s1">&#39;kbest&#39;</span><span class="p">,</span>
            <span class="s1">&#39;k&#39;</span><span class="p">:</span> <span class="n">k</span><span class="p">,</span>
            <span class="s1">&#39;score_func&#39;</span><span class="p">:</span> <span class="n">score_func</span><span class="p">,</span>
            <span class="s1">&#39;scores&#39;</span><span class="p">:</span> <span class="n">scores</span>
        <span class="p">}</span>
    
    <span class="k">elif</span> <span class="n">method</span> <span class="o">==</span> <span class="s1">&#39;rfe&#39;</span><span class="p">:</span>
        <span class="n">estimator</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;estimator&#39;</span><span class="p">)</span>
        <span class="n">n_features</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;n_features_to_select&#39;</span><span class="p">)</span>
        <span class="n">step</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;step&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">selected_indices</span><span class="p">,</span> <span class="n">rankings</span> <span class="o">=</span> <span class="n">recursive_feature_elimination</span><span class="p">(</span>
            <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">estimator</span><span class="o">=</span><span class="n">estimator</span><span class="p">,</span> <span class="n">n_features_to_select</span><span class="o">=</span><span class="n">n_features</span><span class="p">,</span>
            <span class="n">step</span><span class="o">=</span><span class="n">step</span><span class="p">,</span> <span class="n">problem_type</span><span class="o">=</span><span class="n">problem_type</span>
        <span class="p">)</span>
        <span class="n">additional_info</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s1">&#39;method&#39;</span><span class="p">:</span> <span class="s1">&#39;rfe&#39;</span><span class="p">,</span>
            <span class="s1">&#39;n_features_to_select&#39;</span><span class="p">:</span> <span class="n">n_features</span><span class="p">,</span>
            <span class="s1">&#39;step&#39;</span><span class="p">:</span> <span class="n">step</span><span class="p">,</span>
            <span class="s1">&#39;rankings&#39;</span><span class="p">:</span> <span class="n">rankings</span>
        <span class="p">}</span>
    
    <span class="k">elif</span> <span class="n">method</span> <span class="o">==</span> <span class="s1">&#39;boruta&#39;</span><span class="p">:</span>
        <span class="n">max_iter</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;max_iter&#39;</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
        <span class="n">random_state</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;random_state&#39;</span><span class="p">,</span> <span class="mi">42</span><span class="p">)</span>
        <span class="n">selected_indices</span><span class="p">,</span> <span class="n">importances</span> <span class="o">=</span> <span class="n">boruta_feature_selection</span><span class="p">(</span>
            <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">problem_type</span><span class="o">=</span><span class="n">problem_type</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="n">max_iter</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span>
        <span class="p">)</span>
        <span class="n">additional_info</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s1">&#39;method&#39;</span><span class="p">:</span> <span class="s1">&#39;boruta&#39;</span><span class="p">,</span>
            <span class="s1">&#39;max_iter&#39;</span><span class="p">:</span> <span class="n">max_iter</span><span class="p">,</span>
            <span class="s1">&#39;importances&#39;</span><span class="p">:</span> <span class="n">importances</span>
        <span class="p">}</span>
    
    <span class="k">elif</span> <span class="n">method</span> <span class="o">==</span> <span class="s1">&#39;pca&#39;</span><span class="p">:</span>
        <span class="n">n_components</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;n_components&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">variance</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;variance&#39;</span><span class="p">,</span> <span class="mf">0.95</span><span class="p">)</span>
        <span class="n">selected_indices</span><span class="p">,</span> <span class="n">pca_info</span> <span class="o">=</span> <span class="n">apply_pca</span><span class="p">(</span>
            <span class="n">X</span><span class="p">,</span> <span class="n">n_components</span><span class="o">=</span><span class="n">n_components</span><span class="p">,</span> <span class="n">variance</span><span class="o">=</span><span class="n">variance</span><span class="p">,</span> <span class="n">return_indices</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)</span>
        <span class="n">additional_info</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s1">&#39;method&#39;</span><span class="p">:</span> <span class="s1">&#39;pca&#39;</span><span class="p">,</span>
            <span class="s1">&#39;n_components&#39;</span><span class="p">:</span> <span class="n">n_components</span><span class="p">,</span>
            <span class="s1">&#39;variance&#39;</span><span class="p">:</span> <span class="n">variance</span><span class="p">,</span>
            <span class="s1">&#39;pca_info&#39;</span><span class="p">:</span> <span class="n">pca_info</span>
        <span class="p">}</span>
    
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unknown feature selection method: </span><span class="si">{</span><span class="n">method</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">selected_indices</span><span class="p">,</span> <span class="n">additional_info</span></div>



<div class="viewcode-block" id="apply_pca">
<a class="viewcode-back" href="../../../ML_Engine.features.html#ML_Engine.features.selection.apply_pca">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">apply_pca</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">n_components</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">variance</span><span class="o">=</span><span class="mf">0.95</span><span class="p">,</span> <span class="n">return_indices</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Apply Principal Component Analysis.</span>
<span class="sd">    </span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    X : array-like or pandas.DataFrame</span>
<span class="sd">        Feature data</span>
<span class="sd">    n_components : int, optional</span>
<span class="sd">        Number of components. If None, use variance threshold.</span>
<span class="sd">    variance : float, default=0.95</span>
<span class="sd">        Variance to retain (used if n_components is None)</span>
<span class="sd">    return_indices : bool, default=False</span>
<span class="sd">        Whether to return original feature indices (not applicable for PCA)</span>
<span class="sd">    </span>
<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    X_pca : array-like</span>
<span class="sd">        Transformed data</span>
<span class="sd">    pca : PCA</span>
<span class="sd">        Fitted PCA object</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="n">n_components</span><span class="p">)</span>
    
    <span class="k">if</span> <span class="n">n_components</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># Fit PCA to determine number of components for variance threshold</span>
        <span class="n">pca</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">cumulative_variance</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">pca</span><span class="o">.</span><span class="n">explained_variance_ratio_</span><span class="p">)</span>
        <span class="n">n_components</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">cumulative_variance</span> <span class="o">&gt;=</span> <span class="n">variance</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="n">n_components</span><span class="p">)</span>
    
    <span class="n">X_pca</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    
    <span class="k">if</span> <span class="n">return_indices</span><span class="p">:</span>
        <span class="c1"># For PCA, we don&#39;t have original feature indices</span>
        <span class="c1"># Return component indices instead</span>
        <span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_components</span><span class="p">)</span>
        <span class="n">pca_info</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s1">&#39;explained_variance_ratio&#39;</span><span class="p">:</span> <span class="n">pca</span><span class="o">.</span><span class="n">explained_variance_ratio_</span><span class="p">,</span>
            <span class="s1">&#39;components&#39;</span><span class="p">:</span> <span class="n">pca</span><span class="o">.</span><span class="n">components_</span><span class="p">,</span>
            <span class="s1">&#39;n_components&#39;</span><span class="p">:</span> <span class="n">n_components</span><span class="p">,</span>
            <span class="s1">&#39;cumulative_variance&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">pca</span><span class="o">.</span><span class="n">explained_variance_ratio_</span><span class="p">)</span>
        <span class="p">}</span>
        <span class="k">return</span> <span class="n">indices</span><span class="p">,</span> <span class="n">pca_info</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">X_pca</span><span class="p">,</span> <span class="n">pca</span></div>



<div class="viewcode-block" id="generate_x_train_y_train_test">
<a class="viewcode-back" href="../../../ML_Engine.features.html#ML_Engine.features.selection.generate_x_train_y_train_test">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">generate_x_train_y_train_test</span><span class="p">(</span><span class="n">train_df</span><span class="p">,</span> <span class="n">test_df</span><span class="p">,</span> <span class="n">selected_features</span><span class="p">,</span> <span class="n">selected_targets</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generate X_train, X_test, y_train, y_test from train and test dataframes.</span>
<span class="sd">    </span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    train_df : pandas.DataFrame</span>
<span class="sd">        Training dataframe</span>
<span class="sd">    test_df : pandas.DataFrame</span>
<span class="sd">        Test dataframe</span>
<span class="sd">    selected_features : list</span>
<span class="sd">        Names of selected feature columns</span>
<span class="sd">    selected_targets : list</span>
<span class="sd">        Names of target columns</span>
<span class="sd">    </span>
<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    dict</span>
<span class="sd">        Dictionary with keys: X_train, X_test, y_train, y_test, train_df, test_df</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Filter columns that exist in dataframes</span>
    <span class="n">train_features</span> <span class="o">=</span> <span class="p">[</span><span class="n">f</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">selected_features</span> <span class="k">if</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">train_df</span><span class="o">.</span><span class="n">columns</span><span class="p">]</span>
    <span class="n">test_features</span> <span class="o">=</span> <span class="p">[</span><span class="n">f</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">selected_features</span> <span class="k">if</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">test_df</span><span class="o">.</span><span class="n">columns</span><span class="p">]</span>
    <span class="n">train_targets</span> <span class="o">=</span> <span class="p">[</span><span class="n">t</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">selected_targets</span> <span class="k">if</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">train_df</span><span class="o">.</span><span class="n">columns</span><span class="p">]</span>
    <span class="n">test_targets</span> <span class="o">=</span> <span class="p">[</span><span class="n">t</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">selected_targets</span> <span class="k">if</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">test_df</span><span class="o">.</span><span class="n">columns</span><span class="p">]</span>
    
    <span class="n">X_train</span> <span class="o">=</span> <span class="n">train_df</span><span class="p">[</span><span class="n">train_features</span><span class="p">]</span>
    <span class="n">X_test</span> <span class="o">=</span> <span class="n">test_df</span><span class="p">[</span><span class="n">test_features</span><span class="p">]</span>
    <span class="n">y_train</span> <span class="o">=</span> <span class="n">train_df</span><span class="p">[</span><span class="n">train_targets</span><span class="p">]</span>
    <span class="n">y_test</span> <span class="o">=</span> <span class="n">test_df</span><span class="p">[</span><span class="n">test_targets</span><span class="p">]</span>
    
    <span class="n">train_df_filtered</span> <span class="o">=</span> <span class="n">train_df</span><span class="p">[</span><span class="n">train_features</span> <span class="o">+</span> <span class="n">train_targets</span><span class="p">]</span>
    <span class="n">test_df_filtered</span> <span class="o">=</span> <span class="n">test_df</span><span class="p">[</span><span class="n">test_features</span> <span class="o">+</span> <span class="n">test_targets</span><span class="p">]</span>
    
    <span class="k">return</span> <span class="p">{</span>
        <span class="s1">&#39;X_train&#39;</span><span class="p">:</span> <span class="n">X_train</span><span class="p">,</span>
        <span class="s1">&#39;X_test&#39;</span><span class="p">:</span> <span class="n">X_test</span><span class="p">,</span>
        <span class="s1">&#39;y_train&#39;</span><span class="p">:</span> <span class="n">y_train</span><span class="p">,</span>
        <span class="s1">&#39;y_test&#39;</span><span class="p">:</span> <span class="n">y_test</span><span class="p">,</span>
        <span class="s1">&#39;train_df&#39;</span><span class="p">:</span> <span class="n">train_df_filtered</span><span class="p">,</span>
        <span class="s1">&#39;test_df&#39;</span><span class="p">:</span> <span class="n">test_df_filtered</span><span class="p">,</span>
        <span class="s1">&#39;selected_features&#39;</span><span class="p">:</span> <span class="n">train_features</span><span class="p">,</span>
        <span class="s1">&#39;selected_targets&#39;</span><span class="p">:</span> <span class="n">train_targets</span>
    <span class="p">}</span></div>

</pre></div>

          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="Main">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../../../index.html">ML Engine</a></h1>









<search id="searchbox" style="display: none" role="search">
    <div class="searchformwrapper">
    <form class="search" action="../../../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" placeholder="Search"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</search>
<script>document.getElementById('searchbox').style.display = "block"</script><h3>Navigation</h3>
<p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../ML_Engine.html">ML_Engine package</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../../../index.html">Documentation overview</a><ul>
  <li><a href="../../index.html">Module code</a><ul>
  </ul></li>
  </ul></li>
</ul>
</div>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &#169;2024, ACHRAF BEN AHMED.
      
      |
      Powered by <a href="https://www.sphinx-doc.org/">Sphinx 8.1.3</a>
      &amp; <a href="https://alabaster.readthedocs.io">Alabaster 1.0.0</a>
      
    </div>

    

    
  </body>
</html>