{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "dataset-prep-title",
      "metadata": {},
      "source": [
        "# Dataset Preparation: Adult Census Income\n",
        "\n",
        "This notebook downloads and prepares the Adult Census Income dataset for use with the ML_Engine library.\n",
        "\n",
        "## Dataset Information\n",
        "- **Source**: UCI Machine Learning Repository\n",
        "- **Task**: Binary classification (income >50K or <=50K)\n",
        "- **Samples**: ~48,842\n",
        "- **Features**: 14 (mix of numerical and categorical)\n",
        "- **Target**: 'income' column (binary: '>50K' or '<=50K')\n",
        "\n",
        "## Preparation Steps\n",
        "1. Download dataset using scikit-learn's fetch_openml\n",
        "2. Handle missing values\n",
        "3. Encode categorical variables\n",
        "4. Save processed dataset to `dataset/` folder"
      ]
    },
    {
      "cell_type": "code",
      "id": "import-libraries",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Ensure dataset directory exists\n",
        "os.makedirs('../dataset', exist_ok=True)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "id": "download-dataset",
      "metadata": {},
      "source": [
        "# Download Adult Census dataset\n",
        "# This will cache the dataset locally after first download\n",
        "print(\"Downloading Adult Census dataset...\")\n",
        "adult = fetch_openml(name='adult', version=2, as_frame=True)\n",
        "\n",
        "# Extract features and target\n",
        "X = adult.data\n",
        "y = adult.target\n",
        "\n",
        "print(f\"Original dataset shape: {X.shape}\")\n",
        "print(f\"Target shape: {y.shape}\")\n",
        "print(f\"\\nFeature names: {list(X.columns)}\")\n",
        "print(f\"\\nTarget distribution:\")\n",
        "print(y.value_counts())\n",
        "\n",
        "# Display first few rows\n",
        "print(\"\\nFirst 5 rows of features:\")\n",
        "display(X.head())\n",
        "\n",
        "print(\"\\nTarget variable:\")\n",
        "display(y.head())"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "id": "data-exploration",
      "metadata": {},
      "source": [
        "# Data exploration\n",
        "print(\"=== Dataset Information ===\")\n",
        "print(f\"Number of samples: {len(X)}\")\n",
        "print(f\"Number of features: {len(X.columns)}\")\n",
        "\n",
        "# Check data types\n",
        "print(\"\\n=== Data Types ===\")\n",
        "print(X.dtypes)\n",
        "\n",
        "# Check for missing values\n",
        "print(\"\\n=== Missing Values ===\")\n",
        "missing = X.isnull().sum()\n",
        "missing = missing[missing > 0]\n",
        "if len(missing) > 0:\n",
        "    print(missing)\n",
        "else:\n",
        "    print(\"No missing values found.\")\n",
        "\n",
        "# Check categorical columns\n",
        "categorical_cols = X.select_dtypes(include=['object', 'category']).columns\n",
        "print(f\"\\n=== Categorical Columns ({len(categorical_cols)}) ===\")\n",
        "print(list(categorical_cols))\n",
        "\n",
        "# Check numerical columns\n",
        "numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns\n",
        "print(f\"\\n=== Numerical Columns ({len(numerical_cols)}) ===\")\n",
        "print(list(numerical_cols))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "id": "data-preprocessing",
      "metadata": {},
      "source": [
        "# Data preprocessing\n",
        "print(\"Preprocessing data...\")\n",
        "\n",
        "# Create a copy for preprocessing\n",
        "X_processed = X.copy()\n",
        "\n",
        "# 1. Handle missing values (if any) - fill with mode for categorical, median for numerical\n",
        "for col in X_processed.columns:\n",
        "    if X_processed[col].isnull().any():\n",
        "        if X_processed[col].dtype == 'object':\n",
        "            # Fill categorical with mode\n",
        "            X_processed[col].fillna(X_processed[col].mode()[0], inplace=True)\n",
        "        else:\n",
        "            # Fill numerical with median\n",
        "            X_processed[col].fillna(X_processed[col].median(), inplace=True)\n",
        "\n",
        "# 2. Encode target variable\n",
        "y_processed = y.copy()\n",
        "# Map target to binary: '>50K' -> 1, '<=50K' -> 0\n",
        "y_processed = y_processed.map({'<=50K': 0, '>50K': 1})\n",
        "\n",
        "print(f\"Target encoding: {dict(zip(['<=50K', '>50K'], [0, 1]))}\")\n",
        "print(f\"\\nTarget distribution after encoding:\")\n",
        "print(y_processed.value_counts())\n",
        "\n",
        "# 3. For ML_Engine compatibility, we'll:\n",
        "#    - Keep categorical columns as strings (ML_Engine should handle encoding)\n",
        "#    - Or we can one-hot encode them\n",
        "# Let's keep as strings for now, ML_Engine should handle encoding\n",
        "\n",
        "# Combine X and y for saving\n",
        "df_processed = X_processed.copy()\n",
        "df_processed['income'] = y_processed\n",
        "\n",
        "print(f\"\\nProcessed dataset shape: {df_processed.shape}\")\n",
        "print(\"\\nFirst 5 rows of processed data:\")\n",
        "display(df_processed.head())"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "id": "save-dataset",
      "metadata": {},
      "source": [
        "# Save processed dataset\n",
        "output_path = '../dataset/adult_census_processed.csv'\n",
        "df_processed.to_csv(output_path, index=False)\n",
        "\n",
        "print(f\"Dataset saved to: {output_path}\")\n",
        "print(f\"File size: {os.path.getsize(output_path) / 1024 / 1024:.2f} MB\")\n",
        "\n",
        "# Also save a smaller sample for quick testing\n",
        "sample_size = 5000\n",
        "if len(df_processed) > sample_size:\n",
        "    df_sample = df_processed.sample(n=sample_size, random_state=42)\n",
        "    sample_path = '../dataset/adult_census_sample.csv'\n",
        "    df_sample.to_csv(sample_path, index=False)\n",
        "    print(f\"\\nSample dataset ({sample_size} rows) saved to: {sample_path}\")\n",
        "    print(f\"Sample file size: {os.path.getsize(sample_path) / 1024 / 1024:.2f} MB\")\n",
        "\n",
        "# Create a train/test split for consistency\n",
        "train_df, test_df = train_test_split(df_processed, test_size=0.2, random_state=42, stratify=df_processed['income'])\n",
        "train_path = '../dataset/adult_census_train.csv'\n",
        "test_path = '../dataset/adult_census_test.csv'\n",
        "train_df.to_csv(train_path, index=False)\n",
        "test_df.to_csv(test_path, index=False)\n",
        "\n",
        "print(f\"\\nTrain set saved to: {train_path} ({len(train_df)} rows)\")\n",
        "print(f\"Test set saved to: {test_path} ({len(test_df)} rows)\")\n",
        "print(f\"Train target distribution:\\n{train_df['income'].value_counts()}\")\n",
        "print(f\"\\nTest target distribution:\\n{test_df['income'].value_counts()}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "dataset-summary",
      "metadata": {},
      "source": [
        "## Dataset Summary\n",
        "\n",
        "### Files Created:\n",
        "1. `adult_census_processed.csv` - Full processed dataset\n",
        "2. `adult_census_sample.csv` - 5,000 row sample for quick testing\n",
        "3. `adult_census_train.csv` - Training split (80%)\n",
        "4. `adult_census_test.csv` - Test split (20%)\n",
        "\n",
        "### Dataset Characteristics:\n",
        "- **Total samples**: ~48,842\n",
        "- **Features**: 14 original features (mix of numerical and categorical)\n",
        "- **Target**: 'income' (binary: 0 for <=50K, 1 for >50K)\n",
        "- **Missing values**: Handled (filled with mode/median)\n",
        "- **Categorical encoding**: Target encoded to 0/1, features kept as strings\n",
        "\n",
        "### For Use in ML_Engine:\n",
        "The notebooks should use the processed dataset from the `dataset/` folder. For classification tasks, use `income` as the target column."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}